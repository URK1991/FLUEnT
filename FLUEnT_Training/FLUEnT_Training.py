# -*- coding: utf-8 -*-
"""Copy of Video Classification Pipeline meant for download.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SaiKEw6T3wDjNs6fHAQ-C36nSxGLoQ4H

#Video Classification Pipeline
"""

import torch
torch.cuda.is_available()

from google.colab import drive
drive.mount('/content/gdrive')
print("Complete")

import os

import numpy as np
import pandas as pd
import cv2
from google.colab.patches import cv2_imshow

from tqdm.auto import tqdm

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler

from torch.utils.data import Dataset, TensorDataset, DataLoader
import matplotlib.pyplot as plt

from sklearn.metrics import precision_score, recall_score, confusion_matrix, PrecisionRecallDisplay, ConfusionMatrixDisplay
from sklearn.utils.class_weight import compute_class_weight

"""#Data

##Load Data
"""

vae_file = '/content/gdrive/Shareddrives/mBSUS/Umair Specific Material/video_splits/vae_data.pkl'
df = pd.read_pickle(vae_file)
df

"""##data utilities

###function: get_label
"""

# def get_label(file_path, curate_df = curate_df):
#     name = file_path.split('.')[0]
#     if name in list(curate_df['pat_region']): label = 1
#     else: label = 0
#     return label

"""###class: TrainDataset"""

# asdf = ([1,2,3,4,5,6,7,8,9,0,10,0,0,0,0,0])
# next((i for i, x in enumerate(reversed(asdf)) if x), None)

class TrainDataset(Dataset):
    def __init__(self, X, Y, mask_pct=0.00):
        'Initialization'

        X = np.vstack(list(X))
        Y = np.squeeze(np.vstack(list(Y)))

        self.class_weights = torch.Tensor(compute_class_weight(class_weight = 'balanced', 
                                                classes = np.unique(Y), y = Y))

        self.X = torch.Tensor(list(X))
        self.Y = nn.functional.one_hot(torch.Tensor(list(Y)).type(torch.int64), num_classes=2)
        self.mask_pct=mask_pct

    def __random_mask(self, index):# , mask_pct=0.01):
        import random
        X1 = self.X[index]
        if self.mask_pct==0: return X1

        n_frames = len(X1)
        f_shape = X1[0].shape

        pix_sums = X1.sum(axis=1)
        pad_start_idx = n_frames - next((i for i, x in enumerate(reversed(pix_sums)) if x), None)

        mask_idxs = random.sample(range(0, pad_start_idx), int(pad_start_idx*self.mask_pct))
        for idx in mask_idxs:
            X1[idx] = torch.zeros(f_shape)

        return X1
    def __len__(self):
        'Denotes the total number of samples'
        return len(self.X)

    def __getitem__(self, index):
        'Generates one sample of data'
        # Select sample
        X1 = self.__random_mask(index)# , mask_pct=0.01)
        return X1, self.Y[index]

"""#Models

##Position Encoding
"""

!pip install positional_encodings

from positional_encodings.torch_encodings import PositionalEncodingPermute2D, PositionalEncodingPermute1D, Summer

"""## LSTM"""

class LSTM(nn.Module):
  def __init__(self, in_size=400, hidden_size=64, out_size=2, n_layers=1):
      super(LSTM, self).__init__()
      self.in_size = in_size
      self.hidden_size = hidden_size
      self.out_size = out_size
      self.n_layers = n_layers

      self.lstm = nn.LSTM(input_size=in_size, hidden_size=hidden_size, num_layers=n_layers, batch_first=True).to(device)
      self.linear = nn.Linear(hidden_size, out_size).to(device)
      self.softmax = nn.Softmax(dim=0)
      self.sigmoid = nn.Sigmoid()

  def forward(self, x, h):
      batch_size = x.size(0)

      x, h = self.lstm(x, h)
      x = self.linear(x)
      x = self.sigmoid(x)

      x = x.view(batch_size, -1)
      x = x[:,-1]
      return x, h

  def init_hidden(self, batch_size):
      weight = next(self.parameters()).data
      hidden = (weight.new(self.n_layers, batch_size, self.hidden_size).zero_().to(device),
                    weight.new(self.n_layers, batch_size, self.hidden_size).zero_().to(device))

        # return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),
        #         autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))
      return hidden

"""##MLP"""

class MLP(nn.Module):
  '''
    Multilayer Perceptron.
  '''
  def __init__(self, input, output, hidden_dim = 256):
    super().__init__()
    self.layers = nn.Sequential(
        nn.Flatten(),
        nn.Linear(input, hidden_dim),
        nn.LeakyReLU(),
        nn.Linear(hidden_dim, output)
    )

  def forward(self, x):
    '''Forward pass'''
    x = self.layers(x)
    return x

"""##Transformer"""

class Transformer(nn.Module):
    def __init__(self, num_features=1, max_sequence_len = 200, num_layers = 2, num_heads = 8, device = 'cpu'):
        super(Transformer, self).__init__()
        self.num_features = num_features
        self.num_layers = num_layers
        self.num_heads = num_heads
        self.device = device

        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.num_features, nhead=self.num_heads, batch_first = True).to(device)
        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=self.num_layers).to(device)

        self.MLP = MLP(max_sequence_len * num_features, 2)


    def forward(self, X):
        pos_enc = Summer(PositionalEncodingPermute1D(X.shape[1])).to(self.device)
        input = pos_enc(X)
        # flat_layer = nn.Flatten(end_dim=-2)(input) #end=-2 prevents layer from flattening feature dimension
        output = self.transformer_encoder(input)
        output = self.MLP(output)
        output = nn.Softmax()(output)

        return output

"""###Test Self-Attention Model"""

# input = torch.rand((3,200, 500))
# model = Transformer(num_features = 500, num_heads = 10)
# model(input).shape

"""#Training

##Hyperparameters + Helpers
"""

epochs = 20
batch_size = 16
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
lr = 1e-4
momentum = 0.9

# asdf = '/content/gdrive/Shareddrives/mBSUS/Umair Specific Material/model results/SumVecs'
# asdf = os.listdir(asdf)
# result = list(filter(lambda x: 'run' in x, asdf))
# int(sorted(result)[-1].split('n')[-1])

def random_splits(n_samples, n_splits=3, even_splits=True):
  import random
  if even_splits and n_samples%n_splits != 0: return -1

  indices = np.arange(n_samples)
  random.shuffle(indices)

  splits = np.ones(n_samples)
  for i in range(n_samples):
      splits[indices[i]] = int(i%n_splits)

  # for i in range(n_splits):
  #     idxs = indices[i*int(n_samples/n_splits):(i+1)*int(n_samples/n_splits)]
  #     splits[idxs]=i
  return splits

def save_checkpoint_metrics(pth, state, metrics, fig, is_best, m_type, filename):
    """Save checkpoint if a new best is achieved"""
    # gen_pth = '/content/gdrive/Shareddrives/mBSUS/Umair Specific Material/model results/'+m_type

    if is_best:
        # print ("=> Saving a new best")

        # if save_as_run != None: gen_pth = gen_pth + '/run' + save_as_run

        model_pth = os.path.join(pth, 'checkpoints')
        if not os.path.exists(model_pth): os.makedirs(model_pth)
        torch.save(state, model_pth+filename+'.pth.tar')  # save checkpoint

        if metrics is not None:
            metric_df = pd.DataFrame.from_dict(metrics)
            metric_df.columns = ['tp', 'fp', 'tn','fn','sensitivity',
                                 'specificity','balanced_acc', 'loss']
            metric_df.index = [state['epoch']]

            metrics_pth = os.path.join(pth,'metrics')
            if not os.path.exists(metrics_pth): os.makedirs(metrics_pth)

            fname_xval, epoch = filename.rsplit('-', 1)
            metrics_pth = metrics_pth + fname_xval+'.csv'
            metric_df.to_csv(metrics_pth, mode='a', header=not os.path.exists(metrics_pth))
        
        if fig is not None:
            fig_pth = os.path.join(pth,'figs')
            if not os.path.exists(fig_pth): os.makedirs(fig_pth)
            fig.savefig(fig_pth+filename+'.png')
    # else:
        # print ("=> Validation Accuracy did not improve")

def df_from_excels(df, vec_folder, pad_len=400, n_splits=3):
    ## Create df from excels
    df_full = df.copy()
    df_full = df_full.drop(['output_layer','feature_layer'], axis=1)

    target_dir = '/content/gdrive/Shareddrives/mBSUS/Umair Specific Material/'+vec_folder

    # loop through each video
    feature_layers = []
    vid_files = []
    for fname in os.listdir(target_dir):
        xlsx = pd.read_excel(os.path.join(target_dir, fname),index_col=0)
        vid_file = fname.split('.')[0]

        vid_features = [] 
        # get the features for each frame in a single list
        for index, row in xlsx.iterrows():
            vid_features.append(list(row))

        n_features = len(vid_features[0])
        while len(vid_features)<pad_len: vid_features.append([0] * n_features)
        # break
        idx = background_df.index[background_df['vid_file'] == vid_file][0]

        feature_layers.append([vid_features])
        vid_files.append(vid_file)

    # feature_layers=np.array(feature_layers)

    feature_layers = np.asarray(feature_layers)
    # feature_layers = np.expand_dims(feature_layers, axis=1)
    tmp_df = pd.DataFrame({'vid_file': (vid_files), 'feature_layer':list(feature_layers)})
    df_full = df_full.merge(tmp_df, how='left', on='vid_file')
    df = df_full[df_full['split']==1]

    df['cross_val']=random_splits(len(df), n_splits=n_splits, even_splits=False)
    
    return df

"""---

## Train with VAE+ Feature + Position Vec (200 embedding)
"""

feature_file = '/content/gdrive/Shareddrives/mBSUS/Results/Transfer Model Applied to all data/feature_data.pkl'
background_df = pd.read_pickle(feature_file)
background_df

dic = {
    'vid_file':[],
    'vid_dir':[],
    'label':[],
    'predicted_class':[],
    'output_layer':[],
    'feature_layer':[],
    'age':[],
    'rsv':[],
    'cons_type':[]
}

for i in tqdm(list(background_df['vid_file'].unique())):
    try:
        if int(i[4]) == 1: continue
    except:
        continue
    tmp = background_df[background_df.vid_file==i]
    if(len(tmp)>400): continue
    dic['vid_file'] += [i]
    dic['vid_dir'] += [list(tmp['vid_dir'])[0]]
    l = 1 if tmp['label'].sum() > 1 else 0
    # print(f"{l}, {tmp['label'].sum()}")
    dic['label'] += [l]
    p = 1 if tmp['predicted_class'].sum() > 1 else 0
    dic['predicted_class'] += [p]

    OL = tmp['output_layer']
    OL = np.array(list(OL))
    p = 400 - OL.shape[0]
    # p = p if p>0 else 0
    OL = np.pad(OL, ((0,p), (0,0)))
    OL = np.expand_dims(OL, 0)
    # dic['output_layer'] += [np.reshape(np.array(OL), (1,1,len(OL)))]
    dic['output_layer'] += [OL]
                                       
    FL = tmp['feature_layer']
    FL = np.array(list(FL))
    p = 400 - FL.shape[0]
    # p = p if p>0 else 0
    FL = np.pad(FL, ((0,p), (0,0)))
    FL = np.expand_dims(FL, 0)
    # template[1,:,len(FL)] = np.reshape(np.array(FL), (1,1,len(FL)))
    # dic['feature_layer'] += [template+np.reshape(np.array(FL), (1,FL.shape[0],FL.shape[1]))]
    dic['feature_layer'] += [FL]

    dic['age'] += [list(tmp['age'])[0]]
    dic['rsv'] += [list(tmp['rsv'])[0]]
    dic['cons_type'] += [list(tmp['cons_type'].unique())]

df = pd.DataFrame(dic)
df['split'] = df['vid_file'].apply(lambda x: int(x[:3])-71)
background_df = df[df.split >= 0]
background_df

curate_file = '/content/gdrive/Shareddrives/mBSUS/Umair Specific Material/mBSUS_case_ data.xlsx'
curate_df = pd.read_excel(curate_file, sheet_name = 'Curated')
curate_df = curate_df.rename(columns={'Subject ID ':'Subject_ID', 'Age (in months) ': 'Age'})
curate_df

tmp = curate_df[curate_df.Age != 999]
max = tmp['Age'].max()
min = tmp['Age'].min()

vae_file = '/content/gdrive/Shareddrives/mBSUS/Umair Specific Material/video_splits/1x676_LatentVectors.pkl'
df = pd.read_pickle(vae_file)
df

vae_feat_df = {
    'name':[],
    'X':[],
    'Y':[],
    'split':[]
}

L = df['name'].unique()
for i in L:
    D = df[df.name == i]
    A = D['X']
    B = background_df[background_df.vid_file == i]['feature_layer']
    # print(list(A)[0].shape)
    # print(list(B)[0].shape)
    combined_vec = np.concatenate([list(A)[0],list(B)[0]], 2)
    # print(np.concatenate([list(A)[0],list(B)[0]], 2).shape)

    vae_feat_df['name'] += [i]
    vae_feat_df['X'] += [combined_vec]
    vae_feat_df['Y'] += [list(D['Y'])[0]]
    vae_feat_df['split'] += [int(i[:3])-71]

    # break
vae_feat_df = pd.DataFrame(vae_feat_df)
vae_feat_df

vae_feat_pos_df = {
    'name':[],
    'X':[],
    'Y':[],
    'split':[]
}

L = df['name'].unique()
for i in L:
    D = df[df.name == i]
    C = curate_df[curate_df.Subject_ID == i[:5]]
    A = D['X']
    B = background_df[background_df.vid_file == i]['feature_layer']

    r = 1 if i[-3]=='R' else 0 #right
    f = 1 if i[-3]=='L' else 0 #left
    a = 1 if i[-2]=='A' else 0 #anterior
    l = 1 if i[-2]=='L' else 0 #lateral
    p = 1 if i[-2]=='P' else 0 #posterior
    s = 1 if i[-1]=='S' else 0 #sagittal
    t = 1 if i[-1]=='T' else 0 #transverse

    age = list(C['Age'])[0]
    age = (age - min)/(max-min)

    m = np.reshape(np.array([r,f,a,l,p,s,t,age]), (1,1,8))
    m = np.repeat(m, 400, axis = 1)

    combined_vec = np.concatenate([list(A)[0],list(B)[0], m], 2)
    
    vae_feat_pos_df['name'] += [i]
    vae_feat_pos_df['X'] += [combined_vec]
    vae_feat_pos_df['Y'] += [list(D['Y'])[0]]
    vae_feat_pos_df['split'] += [int(i[:3])-71]


vae_feat_pos_df = pd.DataFrame(vae_feat_pos_df)
vae_feat_pos_df

vec_folder = 'vae_pos' # TestVecs_VQVAE SumVecs TestVecs
pad_len = 400
n_splits=3

# df_excels = df_from_excels(background_df, vec_folder, pad_len=pad_len, n_splits=n_splits)

feature_dim = len(list(vae_feat_pos_df['X'])[0][0][0]) 
print(feature_dim)

class Transformer(nn.Module):
    def __init__(self, num_features=1, max_sequence_len = 400, num_layers = 1, num_heads = 8, device = 'cuda', vae_dim=None, feat_dim=None, meta_dim=None):
        super(Transformer, self).__init__()
        self.num_features = num_features
        self.num_layers = num_layers
        self.num_heads = num_heads
        self.device = device
        self.vae_dim = vae_dim
        self.feat_dim = feat_dim
        self.meta_dim = meta_dim

        self.vae_layer = nn.Sequential(nn.Linear(vae_dim, 200), nn.ReLU()) if vae_dim is not None else None
        self.feat_layer = nn.Sequential(nn.Linear(feat_dim, 200), nn.ReLU()) if feat_dim is not None else None
        self.meta_layer = nn.Sequential(nn.Linear(meta_dim, 200), nn.ReLU()) if meta_dim is not None else None

        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.num_features, nhead=self.num_heads, batch_first = True).to(device)
        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=self.num_layers).to(device)

        self.MLP = MLP(max_sequence_len * num_features, 2)


    def forward(self, X):
        dim_tracker = 0
        v, f, m = [],[],[]
        if self.vae_layer is not None:
            v = self.vae_layer(X[:,:,:self.vae_dim])
            dim_tracker += self.vae_dim
        if self.feat_layer is not None:
            f = self.feat_layer(X[:,:,dim_tracker:dim_tracker+self.feat_dim])
            dim_tracker += self.feat_dim
        if self.meta_layer is not None:
            m = self.meta_layer(X[:,:,dim_tracker:dim_tracker+self.meta_dim])
            dim_tracker += self.meta_dim

        X = torch.concat([v, f, m], 2)
        
        pos_enc = Summer(PositionalEncodingPermute1D(X.shape[1])).to(self.device)
        input = pos_enc(X)
        # flat_layer = nn.Flatten(end_dim=-2)(input) #end=-2 prevents layer from flattening feature dimension
        output = self.transformer_encoder(input)
        output = self.MLP(output)

        return output

feature_dim = len(list(vae_feat_pos_df['X'])[0][0][0]) # list(df['X'])[0].shape[-1] # df['feature_layer'][0].shape[-1]
print(feature_dim)

model_type = 'Transformer'
vec_folder = 'VAE+position'
n_splits=3
batch_size = 16
epochs = 30

save_as_run=True

gen_pth = '/content/gdrive/Shareddrives/mBSUS/Umair Specific Material/model results/'+model_type+'/'+vec_folder
if not os.path.exists(gen_pth): os.makedirs(gen_pth)

PATH = '/content/gdrive/Shareddrives/mBSUS/Models/Video Classifiers/saved_models/vae+feat+meta.pth'

if save_as_run:
    runs_list = list(filter(lambda x: 'run' in x, os.listdir(gen_pth)))
    if len(runs_list) == 0: n_runs=0
    else: n_runs = int(sorted(runs_list)[-1].split('n')[-1])
    gen_pth = gen_pth + '/run' + str(n_runs+1)


# df['split']=random_splits(len(df), n_splits=n_splits, even_splits=False)

best_balanced = []
best_sens = []
best_spec = []
best_F1s = []
BBA = 0
BSE = 0
BSP = 0
BF1 = 0

for i in tqdm(list(vae_feat_pos_df['split'].unique())):
    if vae_feat_pos_df[vae_feat_pos_df.split == i].shape[0]<10: continue
    # try:
    test = vae_feat_pos_df[vae_feat_pos_df.split == i]
    train = vae_feat_pos_df[vae_feat_pos_df.split != i]

    dataset = TrainDataset(train['X'], train['Y'], mask_pct=0)
    train_dataloader = DataLoader(dataset,
            batch_size = batch_size,
            shuffle = True)
    class_weights = dataset.class_weights

    dataset = TrainDataset(test['X'], test['Y'], mask_pct=0)
    test_dataloader = DataLoader(dataset,
            batch_size = batch_size,
            shuffle = False)
    
    training_loss, testing_loss, F1, Balanced = [], [], [], []

    # criterion = nn.BCELoss(weight=class_weights.to(device),reduction='mean')
    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device),reduction='mean')
    # model = LSTM(in_size=feature_dim, hidden_size=256, out_size=2, n_layers=1)
    model = Transformer(num_features = 600, 
                        max_sequence_len = 400, 
                        num_heads = 15, 
                        num_layers = 1,
                        vae_dim = 676,
                        feat_dim = 512,
                        meta_dim = 8,
                        ).to(device)
    # optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, dampening=0, weight_decay=0.0001)
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.0001)
    
    best_accuracy=float('-inf')

    for e in tqdm(range(epochs)):
        # h = model.init_hidden(batch_size)

        losses = 0
        for videos, labels in train_dataloader:
            # h = tuple([e.data for e in h])

            model.train()
            optimizer.zero_grad()
            videos = videos.to(device)
            labels = labels.to(device)
            # output, h = model(videos, h)
            output = model(videos)
            loss = criterion(output.float(), labels.float())
            loss.backward()
            optimizer.step()
            losses += loss.item()

        training_loss += [losses/len(train_dataloader)]

        
        with torch.no_grad():
            # val_h = model.init_hidden(batch_size)
            losses = 0
            actuals, preds = [], []
            for videos, labels in test_dataloader:
                # val_h = tuple([each.data for each in val_h])
                model.eval()
                videos = videos.to(device)
                labels = labels.to(device)

                # output, val_h = model(videos, val_h)
                output = model(videos)

                preds.append(output.cpu().numpy())
                actuals.append(labels.cpu().numpy())
                
                try:
                    loss = criterion(output.float(), labels.float())
                except:
                    print(output.shape)
                    print(labels.shape)

                losses += loss.item()

            testing_loss += [losses/len(test_dataloader)]

        A = np.argmax(np.vstack(actuals), axis = 1)
        P = np.argmax(np.vstack(preds), axis = 1)
        matrix = confusion_matrix(A, P, labels = [0,1])

        Tp = matrix[1,1]
        Fp = matrix[0,1]
        Tn = matrix[0,0]
        Fn = matrix[1,0]
        F1 += [Tp/(Tp + 0.5*(Fp+Fn))]

        sense = (Tp / (Tp + Fn)) if (Tp + Fn) != 0 else 0
        speci = (Tn / (Tn + Fp)) if (Tn + Fp) != 0 else 0
        balanced_acc = (sense+speci)/2 if (Tp + Fn) != 0 else speci
        Balanced += [balanced_acc]

        # is_best = bool(balanced_acc > best_accuracy)
        # best_accuracy = balanced_acc if balanced_acc > best_accuracy else best_accuracy

        is_best = bool(testing_loss[-1] < best_accuracy)
        best_accuracy = balanced_acc if testing_loss[-1] > best_accuracy else best_accuracy

        save_checkpoint_metrics(gen_pth,
            {
            'epoch': e,
            'state_dict': model.state_dict(),
            'best_accuracy': best_accuracy
            }, 
            {
            'tp': [Tp],
            'fp': [Fp],
            'tn': [Tn],
            'fn': [Fn],
            'sensitivity': [sense],
            'specificity': speci,
            'balanced_acc': balanced_acc,
            'loss': testing_loss[-1],
        }, fig = None, is_best=is_best, m_type=vec_folder, 
            filename=('/'+vec_folder+'-xval'+str(i)+'-e'+str(e)))
        
        if is_best: 
            BBA = balanced_acc
            BSE = sense
            BSP = speci
            BF1 = F1[-1]
            torch.save(model, PATH)
    

    f, ax = plt.subplots(1,3, figsize=(12, 4), layout = 'constrained')
    ax[0].plot(training_loss, label = 'Training Loss')
    ax[0].plot(testing_loss, label = 'Testing Loss')
    ax[0].legend()

    ax[1].plot(F1, label = 'F1 Score')
    ax[1].plot(Balanced, label = 'Balanced Accuracy')
    ax[1].legend()

    ConfusionMatrixDisplay(matrix, display_labels = ['NP', 'P']).plot(ax=ax[2])

    plt.show()     
    
    best_balanced += [BBA]
    best_sens += [BSE]
    best_spec += [BSP]
    best_F1s += [BF1]

    break


print(f'best balanced: {best_balanced}')
print(f'best_balanced mean: {np.mean(best_balanced)}')
print()
print(f'best F1: {best_F1s}')
print(f'best F1 mean: {np.mean(best_F1s)}')

!pip install grad-cam

from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget
from pytorch_grad_cam.utils.image import show_cam_on_image
from torchvision.models import resnet50

model_path = '/content/gdrive/Shareddrives/mBSUS/Models/Video Classifiers/saved_models/vae+feat+meta.pth'
FLUEnT = torch.load(model_path)
FLUEnT

test_dataloader = DataLoader(dataset,
            batch_size = 1,
            shuffle = False)

videos, labels = next(iter(test_dataloader))
input = A[0][0].unsqueeze(0).shape
# FLUEnT(videos)
FLUEnT.eval()
videos = videos.to(device)
labels = labels.to(device)

# output, val_h = model(videos, val_h)
output = FLUEnT(videos)
output.shape

target_layers = [FLUEnT.encoder_layer]
cam = GradCAM(model=FLUEnT, target_layers=target_layers, use_cuda=True)
targets = [ClassifierOutputTarget(2)]

grayscale_cam = cam(input_tensor=videos, targets=targets)

# In this example grayscale_cam has only one image in the batch:
grayscale_cam = grayscale_cam[0, :]
visualization = show_cam_on_image(videos, grayscale_cam, use_rgb=True)

import matplotlib.pyplot as plt
import numpy as np; np.random.seed(1)
plt.rcParams["figure.figsize"] = 5,2

x = np.linspace(-3,3)
y = np.cumsum(np.random.randn(50))+6

fig, (ax,ax2) = plt.subplots(nrows=2, sharex=True)

extent = [x[0]-(x[1]-x[0])/2., x[-1]+(x[1]-x[0])/2.,0,1]
ax.imshow(y[np.newaxis,:], cmap="plasma", aspect="auto", extent=extent)
ax.set_yticks([])
ax.set_xlim(extent[0], extent[1])

ax2.plot(x,y)

plt.tight_layout()
plt.show()

"""

---

"""
