# -*- coding: utf-8 -*-
"""One Time Train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rg60WbCrolnQnPTFzeBqBVMgB3i1rLh-
"""

from google.colab import drive
drive.mount('/content/gdrive')
print("Complete")

# Commented out IPython magic to ensure Python compatibility.
import os
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE"

import numpy as np
import pandas as pd
import cv2
from google.colab.patches import cv2_imshow
from google.colab import files

# %matplotlib inline
# from __future__ import print_function, division

from tqdm.auto import tqdm

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import torch.backends.cudnn as cudnn
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
from torch.utils.data import Dataset, TensorDataset, DataLoader
from torch.nn.functional import one_hot
from torch.nn.modules import activation

import matplotlib.pyplot as plt
from torchsummary import summary
import time
import os
import copy

from sklearn.metrics import precision_score, recall_score, confusion_matrix, PrecisionRecallDisplay, ConfusionMatrixDisplay
from sklearn.utils.class_weight import compute_class_weight

"""#Data"""

curate_data = '/content/gdrive/Shareddrives/mBSUS/Umair Specific Material/curate_data_withloc.csv'
curate_data_df = pd.read_csv(curate_data)
curate_data_df

curate_data_df['pat_region']

"""##get_pneu_type"""

def get_pneu_type(v):
    pneu_frames = []
    cons_types = []
    cons_map = {}

    temp_df = curate_data_df[curate_data_df.pat_region == v[:-4]]
    for index, i in temp_df.iterrows():
        start = int(i[2])
        end = int(i[3])
        p_list = list(range(start, end))

        pneu_frames += p_list
        cons_types += [i[4] for p in p_list]
        d = {k:v for (k,v) in zip(p_list, cons_types)}
        cons_map.update(d)

    return pneu_frames, cons_map

"""##Video Data"""

vids = '/content/gdrive/Shareddrives/mBSUS/Umair Specific Material/mBSUS_(case)_videos'

source_list = os.listdir(vids)
vid_list = list(curate_data_df['pat_region'])
#filter list
vid_list = [i for i in vid_list if int(i[:3])<=70]

frames = []
labels = []

for v in tqdm(vid_list):
    v = v+'.mp4'
    if v not in source_list:
        print(f'{v} not found')
        continue

    vid_path = os.path.join(vids, v)

    cap = cv2.VideoCapture(vid_path)

    pneu_frames, cons_map = get_pneu_type(v)

    frame_num = 0
    while(cap.isOpened()):
        ret, frame = cap.read()
        if not ret: break
        frame = cv2.resize(frame, (256,256))
        # cv2_imshow(frame)

        # frame_tensor = torch.Tensor(frame).permute(2,0,1).unsqueeze(0)
        frames += [frame]

        label = 1 if frame_num in pneu_frames else 0
        labels += [label]


        frame_num += 1

    cap.release()
    cv2.destroyAllWindows()

len(frames)

"""##TrainDataset"""

class TrainDataset(Dataset):
    def __init__(self, X, Y):
        'Initialization'
        self.X = X
        self.Y = Y

        self.transform = transforms.Compose([
            # transforms.ToPILImage(),
            # transforms.RandomChoice([
            transforms.RandomHorizontalFlip(0.5),
            transforms.RandomRotation(degrees = (-45, 45)),
            # transforms.RandomPerspective(distortion_scale=0.6, p=0.5),
            transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),
            transforms.ElasticTransform(alpha=50.0),
            transforms.RandomAdjustSharpness(sharpness_factor=2),
            # ]),

            # transforms.ToTensor(),
            ])

    def __len__(self):
        'Denotes the total number of samples'
        return len(self.X)

    def __getitem__(self, index):
        'Generates one sample of data'
        # Select sample
        X1 = self.X[index]

        X2 = self.transform(X1)
        return X2, self.Y[index]

frames = np.array(frames)
labels = np.array(labels)

class_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(labels), y = labels)

train_x_tensor = torch.Tensor(cv2.normalize(frames, None, 0.0, 1.0, cv2.NORM_MINMAX, dtype=cv2.CV_32F)).permute(0,3,1,2)
train_y_tensor = nn.functional.one_hot(torch.Tensor(labels).type(torch.int64))

dataset = TrainDataset(train_x_tensor, train_y_tensor)

class_weights

"""#Models

##Model Class
"""

def conv3x3(in_planes, out_planes, stride=1):
    "3x3 convolution with padding"
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=1, bias=False)


class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()

        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return self.sigmoid(x)

class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = nn.BatchNorm2d(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = nn.BatchNorm2d(planes)

       # self.ca = ChannelAttention(planes)
        self.sa = SpatialAttention()

        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

      #  out = self.ca(out) * out
        out = self.sa(out) * out

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
                               padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * 4)
        self.relu = nn.ReLU(inplace=True)

       # self.ca = ChannelAttention(planes * 4)
        self.sa = SpatialAttention()

        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

       # out = self.ca(out) * out
        out = self.sa(out) * out

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class ResNet(nn.Module):

    def __init__(self, block, layers, num_classes=4):
        self.inplanes = 64
        super(ResNet, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,
                               bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.dp = nn.Dropout(0.5)
        self.fc = nn.Linear(512 * block.expansion, 4)


        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion),
            )

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.dp(x)
        x = self.fc(x)

        return x

def resnet18_cbam(pretrained=False, **kwargs):
    """Constructs a ResNet-18 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)
    
    return model

"""##Load Models"""

def load_model(model_path):
    #Construct Model
    model = torch.load(model_path)
   
    num_ftrs = model.fc.in_features
    model.fc = nn.Sequential(
            nn.Linear(num_ftrs, 2),
            # nn.Sigmoid()
            nn.Softmax()
    ) 
    print(model.fc)

    return model

resatten_CL_2711 = '/content/gdrive/Shareddrives/mBSUS/Models/Classifiers/Outside Work/resatten_CL_2711.pth'

"""#Train"""

epochs = 100
batch_size = 64

dataloader = DataLoader(dataset,
            batch_size = batch_size,
            shuffle = True)

class_weights = torch.Tensor(class_weights)
criterion = nn.CrossEntropyLoss(weight=class_weights.to('cuda'),reduction='mean')

# model = torch.load(model_path)
modelv = resatten_CL_2711
model = load_model(modelv)
model = model.to('cuda')
optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, dampening=0, weight_decay=0.0001)

#Training Loop
training_loss = []
for epoch in tqdm(range(1, epochs + 1)):
    training_loss = []
    for images, labels in dataloader:
        optimizer.zero_grad()
        images = images.type(torch.FloatTensor ).to('cuda')
        labels = torch.Tensor(labels)
        labels = labels.to(torch.int64).type(torch.FloatTensor ).to('cuda')
        model.train()
        output = model(images)
        
        loss = criterion(output, labels)
        loss.backward()
        optimizer.step()

        training_loss += [loss.item()]

file_name = "Dumb_File_Name.pth"
dir = '/content/gdrive/Shareddrives/mBSUS/Models/Classifiers/Outside Work/Model Weights/'
path = dir + file_name
torch.save(model, path)
